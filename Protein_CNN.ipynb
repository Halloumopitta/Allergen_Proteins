{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91bcfd0",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "1. Initially \"criterion = nn.CrossEntropyLoss()\" that expects torch.long numbers to be parsed, but our 2nd mapping uses float numbers instead so now a Binary cross entropy function is used together with a sigmoid function to flatten the outputs between 0 and 1\n",
    "2. Our BCE criterion also requires a sigmoid layer at the output to squash the output between 0 and 1 and also the unsqueeze(1) command so that the shape of our data and targets match the output (e.g torch.Size([ batch_size]) becomes              torch.Size([ batch_size,1])  ) \n",
    "\n",
    "\n",
    "references: \n",
    "1. https://medium.com/analytics-vidhya/simple-neural-network-with-bceloss-for-binary-classification-for-a-custom-dataset-8d5c69ffffee\n",
    "2. https://saturncloud.io/blog/using-weights-in-crossentropyloss-and-bceloss-pytorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe497c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import torchvision.transforms as transforms  \n",
    "import torchvision\n",
    "from torch.utils.data import (Dataset,DataLoader)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b99c3d",
   "metadata": {},
   "source": [
    "# Allergen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5753e377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins left: 1965\n",
      "Minimum length: 5\n",
      "Maximum length: 750\n",
      "Mean: 289.0086513994911\n",
      "Median: 255.0\n",
      "Mode: 134\n"
     ]
    }
   ],
   "source": [
    "#Define a class to  filter out  the unwanted proteins\n",
    "def filter_fasta(input_file, output_file, min_length, max_length):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    filtered_proteins = []\n",
    "    filtered_lengths = []\n",
    "\n",
    "    current_protein = None\n",
    "    current_sequence = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            if current_protein is not None:\n",
    "                sequence = ''.join(current_sequence)\n",
    "                sequence_length = len(sequence)\n",
    "                if min_length <= sequence_length <= max_length:\n",
    "                    filtered_proteins.append(current_protein + '\\n' + sequence)\n",
    "                    filtered_lengths.append(sequence_length)\n",
    "            current_protein = line.strip()\n",
    "            current_sequence = []\n",
    "        else:\n",
    "            current_sequence.append(line.strip())\n",
    "\n",
    "    # Check the last protein after the loop ends\n",
    "    if current_protein is not None:\n",
    "        sequence = ''.join(current_sequence)\n",
    "        sequence_length = len(sequence)\n",
    "        if min_length <= sequence_length <= max_length:\n",
    "            filtered_proteins.append(current_protein + '\\n' + sequence)\n",
    "            filtered_lengths.append(sequence_length)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(filtered_proteins))\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_proteins = len(filtered_proteins)\n",
    "    min_length = min(filtered_lengths)\n",
    "    max_length = max(filtered_lengths)\n",
    "    mean = np.mean(filtered_lengths)\n",
    "    median= np.median(filtered_lengths)\n",
    "    mode = stats.mode(filtered_lengths)\n",
    "\n",
    "    # Plot distribution\n",
    "    plt.hist(filtered_lengths, bins=50)\n",
    "    plt.xlabel('Protein length')\n",
    "    plt.ylabel('Number of proteins')\n",
    "    plt.title('Histogram of filtered allergen proteins')\n",
    "    plt.show()\n",
    "\n",
    "    return total_proteins, min_length, max_length, mean, median, mode\n",
    "\n",
    "# Usage\n",
    "total, minimum, maximum,mean,median,mode = filter_fasta('Allergen_Proteins.fasta', 'Filtered_Allergen_Proteins.fasta', 1, 750)\n",
    "print(\"Total proteins left:\", total)\n",
    "print(\"Minimum length:\", minimum)\n",
    "print(\"Maximum length:\", maximum)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Mode:\", mode[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4257a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sequence(sequence, mapping):\n",
    "    map = [mapping[aa] for aa in sequence if aa in mapping]\n",
    "    return np.array(map)\n",
    "\n",
    "def map_fasta(input_file, mapping):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    mapped_sequences = []\n",
    "\n",
    "    current_protein_id = None\n",
    "    current_sequence = ''\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            if current_protein_id and current_sequence:\n",
    "                mapped_sequence = map_sequence(current_sequence, mapping)\n",
    "                mapped_sequences.append((current_protein_id, mapped_sequence))\n",
    "            current_protein_id = line.strip()[1:]\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "\n",
    "    # Process the last protein sequence\n",
    "    if current_protein_id and current_sequence:\n",
    "        mapped_sequence = map_sequence(current_sequence, mapping)\n",
    "        mapped_sequences.append((current_protein_id, mapped_sequence))\n",
    "\n",
    "    return mapped_sequences\n",
    "\n",
    "# Mapping of amino acids to numbers\n",
    "mapping= {\n",
    "    'A': -0.21, 'C': -6.04, 'D': 1.36, 'E': 2.3, 'F': -4.65, 'G': 0, 'H': -1.23,\n",
    "    'I': -4.81, 'K': 3.88, 'L': -4.68, 'M': -3.66, 'N': 0.96, 'P': 0.75, 'Q': 1.52,\n",
    "    'R': 2.11, 'S': 1.74, 'T': 0.78, 'V': -3.5, 'W': -3.32, 'Y': -1.01,\n",
    "    'X': 0, 'U': 0, 'B': 0, 'J': 0, 'O': 0, 'Z': 0\n",
    "}\n",
    "\n",
    "# Input FASTA file path\n",
    "input_file = 'Filtered_Allergen_Proteins.fasta'\n",
    "\n",
    "\n",
    "mapped_sequences = map_fasta(input_file, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96fce5",
   "metadata": {},
   "source": [
    "# Unkwown proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b539035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5000 proteins and saved them as 'Reduced_NOT_Allergen_Proteins.fasta'.\n"
     ]
    }
   ],
   "source": [
    "input_file = 'NOT_Allergen_Proteins.fasta'\n",
    "output_file = 'Reduced_NOT_Allergen_Proteins.fasta'\n",
    "selected = 5000\n",
    "\n",
    "\n",
    "proteins = list(SeqIO.parse(input_file, 'fasta'))\n",
    "\n",
    "\n",
    "selected_proteins = random.sample(proteins, selected)\n",
    "\n",
    "# Write the selected protein records to the output file\n",
    "SeqIO.write(selected_proteins, output_file, 'fasta')\n",
    "\n",
    "print(f\"Selected {selected} proteins and saved them as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0f4d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins left: 4609\n",
      "Minimum length: 8\n",
      "Maximum length: 749\n",
      "Mean: 298.6934259058364\n",
      "Median: 276.0\n",
      "Mode: 173\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "total, minimum, maximum, mean,median,mode = filter_fasta('Reduced_NOT_Allergen_Proteins.fasta', 'Filtered_NOT_Allergen_Proteins.fasta', 1,750)\n",
    "print(\"Total proteins left:\", total)\n",
    "print(\"Minimum length:\", minimum)\n",
    "print(\"Maximum length:\", maximum)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Mode:\", mode[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da638d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert amino acid sequences to numbers\n",
    "def map_sequence(sequence, mapping):\n",
    "    map = [mapping[aa] for aa in sequence if aa in mapping]\n",
    "    return np.array(map)\n",
    "\n",
    "def map_fasta(input_file, mapping):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    mapped_sequences = []\n",
    "\n",
    "    current_protein_id = None\n",
    "    current_sequence = ''\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            if current_protein_id and current_sequence:\n",
    "                mapped_sequence = map_sequence(current_sequence, mapping)\n",
    "                mapped_sequences.append((current_protein_id, mapped_sequence))\n",
    "            current_protein_id = line.strip()[1:]\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "\n",
    "    # Process the last protein sequence\n",
    "    if current_protein_id and current_sequence:\n",
    "        mapped_sequence = map_sequence(current_sequence, mapping)\n",
    "        mapped_sequences.append((current_protein_id, mapped_sequence))\n",
    "\n",
    "    return mapped_sequences\n",
    "\n",
    "# Mapping of amino acids to numbers\n",
    "mapping= {\n",
    "    'A': -0.21, 'C': -6.04, 'D': 1.36, 'E': 2.3, 'F': -4.65, 'G': 0, 'H': -1.23,\n",
    "    'I': -4.81, 'K': 3.88, 'L': -4.68, 'M': -3.66, 'N': 0.96, 'P': 0.75, 'Q': 1.52,\n",
    "    'R': 2.11, 'S': 1.74, 'T': 0.78, 'V': -3.5, 'W': -3.32, 'Y': -1.01,\n",
    "    'X': 0, 'U': 0, 'B': 0, 'J': 0, 'O': 0, 'Z': 0\n",
    "}\n",
    "\n",
    "# Input FASTA file path\n",
    "input_file = 'Filtered_NOT_Allergen_Proteins.fasta'\n",
    "\n",
    "\n",
    "mapped_sequences = map_fasta(input_file, mapping)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c9d5c",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9dca741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of proteins: 6574\n",
      "Total number of allergen proteins: 1965\n",
      "Total number of non allergen proteins: 4609\n",
      "Max length: 750\n",
      "First 5 proteins:\n",
      "Label: 0\n",
      "Sequence: MACARPLTGRTLPADESPCLTLILEPVSGEAAKNSTPVVVDKPGKPPPKRHRRQYNVTVSCNDCDKRLNFSVKTTCSTILTLQQLLTEDLDFLCSFCEAKNGXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "Label: 1\n",
      "Sequence: MKPPIALACLCLLVPLAGGNLVQFGVMIERMTGKPALQYNDYGCYCGVGGSHWPVDETDWCCHAHDCCYGRLEKLGCDPKLEKYLFSITRDNIFCAGRTACQRHTCECDKRAALCFRHNLNTYNRKYAHYPNKLCTGPTPPCXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "Label: 0\n",
      "Sequence: MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVTGQIKADFKRTSWEAKAEGSLEPGRLKLQLASIVPLYSSLVTAGPASKIIILKRTSLPTVSPSNERAYLLPVSFTDLAHVFYLSYFSINAKSNSFSLDIIIALGIPHNTQAHFNHXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "Label: 1\n",
      "Sequence: MIFTNNIAAFQNVVLVKKVKIVLLIFYGSIMFSMTQVNKEECDYYQNLNLGEIYYIYNPRYPLPYSGSKCTWTITSYHRINLKCSLVEFSENKNCNAGSLTVKKNFANKYCGNITLNIESTSNKMTVILTPPGRFFCEVRPIKRVKDSTNCNCGWKNPSRIVGGTNTGINEFPMMAGIKRTYEPGMICGATIISKRYVLTAAHCIIDENTTKLAIVVGEHDWSSKTETNATVLHSINKVIIHPKYDIIEKDDWQINDIALLKTEKDIKFGDKVGPACLPFQHFLDSFAGSDVTVLGWGHTSFNGMLSHILQKTTLNMLTQVECYKYYGNIMVNAMCAYAKGKDACQMDSGGPVLWQNPRTKRLVNIGIISWGAECGKYPNGNTKVGSYIDWIVSQTPDAEYCVIEXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "Label: 0\n",
      "Sequence: MTKIAIIDYGMGNLRSVQKGFEKVGFEAVVTADPKVVLEAEKIVLPGVGAFRDCMRNLEQGGFVEPILRVIREGRPFLGICVGMQLLLTDSVEFGLYQGLNVIPGHVLRFPEGMREGGEELKVPHMGWNQLSIKRRPPAFAEVEDGANVYFVHSYYEMPDDESVIAATCTYGVEFCAAIWKDNIVATQFHPEKSQAVGLSILKNFGEMKXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sequence_padding(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padding_length = max_length - len(seq)\n",
    "        padded_seq = seq + \"X\" * padding_length  # Padding with a special token, \"X\"\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences\n",
    "\n",
    "allergen_sequences = []\n",
    "allergen_headers = []\n",
    "\n",
    "with open(\"Filtered_Allergen_Proteins.fasta\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\">\"):\n",
    "        allergen_headers.append(line.strip())\n",
    "    else:\n",
    "        allergen_sequences.append(line.strip())\n",
    "        \n",
    "\n",
    "non_allergen_sequences = []\n",
    "non_allergen_headers = []\n",
    "\n",
    "with open(\"Filtered_NOT_Allergen_Proteins.fasta\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\">\"):\n",
    "        non_allergen_headers.append(line.strip())\n",
    "    else:\n",
    "        non_allergen_sequences.append(line.strip())\n",
    "\n",
    "# Step 2: Create a merged dataset\n",
    "merged_sequences = allergen_sequences + non_allergen_sequences\n",
    "merged_labels = [1] * len(allergen_sequences) + [0] * len(non_allergen_sequences)\n",
    "\n",
    "# Shuffle the protein order\n",
    "merged_data = list(zip(merged_sequences, merged_labels))\n",
    "random.shuffle(merged_data)\n",
    "merged_sequences, merged_labels = zip(*merged_data)\n",
    "\n",
    "# Step 3: Determine the maximum sequence length for padding\n",
    "max_length = max(len(seq) for seq in merged_sequences)\n",
    "\n",
    "# Step 4: Apply sequence padding\n",
    "padded_sequences = sequence_padding(merged_sequences, max_length)\n",
    "\n",
    "# Step 5: Save the merged dataset with padded sequences\n",
    "with open(\"Proteins.fasta\", \"w\") as file:\n",
    "    for i in range(len(padded_sequences)):\n",
    "        file.write(\"> Label:\" + str(merged_labels[i]) + \"\\n\")\n",
    "        file.write(padded_sequences[i] + \"\\n\")\n",
    "\n",
    "# Print the total number of proteins\n",
    "total_proteins = len(padded_sequences)\n",
    "print(\"Total number of proteins:\", total_proteins)\n",
    "\n",
    "\n",
    "label_1 = sum(label == 1 for label in merged_labels)\n",
    "label_0 = sum(label == 0 for label in merged_labels)\n",
    "\n",
    "print(\"Total number of allergen proteins:\", label_1)\n",
    "print(\"Total number of non allergen proteins:\", label_0)\n",
    "print(\"Max length:\",max_length)\n",
    "# Print the first 5 proteins\n",
    "print(\"First 5 proteins:\")\n",
    "for i in range(5):\n",
    "    print(\"Label:\", merged_labels[i])\n",
    "    print(\"Sequence:\", padded_sequences[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a2f981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n",
      "Sequence: MACARPLTGRTLPADESPCLTLILEPVSGEAAKNSTPVVVDKPGKPPPKRHRRQYNVTVSCNDCDKRLNFSVKTTCSTILTLQQLLTEDLDFLCSFCEAKNGXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "\n",
      "Total number of sequences: 6574\n",
      "First mapped sequence: [-3.66, -0.21, -6.04, -0.21, 2.11, 0.75, -4.68, 0.78, 0, 2.11, 0.78, -4.68, 0.75, -0.21, 1.36, 2.3, 1.74, 0.75, -6.04, -4.68, 0.78, -4.68, -4.81, -4.68, 2.3, 0.75, -3.5, 1.74, 0, 2.3, -0.21, -0.21, 3.88, 0.96, 1.74, 0.78, 0.75, -3.5, -3.5, -3.5, 1.36, 3.88, 0.75, 0, 3.88, 0.75, 0.75, 0.75, 3.88, 2.11, -1.23, 2.11, 2.11, 1.52, -1.01, 0.96, -3.5, 0.78, -3.5, 1.74, -6.04, 0.96, 1.36, -6.04, 1.36, 3.88, 2.11, -4.68, 0.96, -4.65, 1.74, -3.5, 3.88, 0.78, 0.78, -6.04, 1.74, 0.78, -4.81, -4.68, 0.78, -4.68, 1.52, 1.52, -4.68, -4.68, 0.78, 2.3, 1.36, -4.68, 1.36, -4.65, -4.68, -6.04, 1.74, -4.65, -6.04, 2.3, -0.21, 3.88, 0.96, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Length of each sequence: 750\n",
      "6574\n",
      "Numeric sequences and labels saved to Mapped.fasta\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################\n",
    "\n",
    "#Create a class that takes the encoded sequences and labels to split later for train,val and test\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "        return sequence_tensor, label_tensor\n",
    "\n",
    "    \n",
    "################################################################################################################################\n",
    "\n",
    "################################################################################################################################\n",
    "# Load the merged dataset with padded sequences and verify it works by printing the first sequence\n",
    "merged_sequences = []\n",
    "merged_labels = []\n",
    "\n",
    "with open(\"Proteins.fasta\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i in range(0, len(lines),2):\n",
    "    sequence = lines[i+1].strip() #check every odd line and strip any whitespace\n",
    "    label = int(lines[i].split(\":\")[1]) #check every even line\n",
    "    merged_sequences.append(sequence)\n",
    "    merged_labels.append(label)\n",
    "    \n",
    "for i in range(1):\n",
    "    print(f\"Label: {merged_labels[i]}\")\n",
    "    print(f\"Sequence: {merged_sequences[i]}\")\n",
    "    print()\n",
    "\n",
    "################################################################################################################################\n",
    "#Map sequences to numbers again and save the file\n",
    "mapping= {\n",
    "    'A': -0.21, 'C': -6.04, 'D': 1.36, 'E': 2.3, 'F': -4.65, 'G': 0, 'H': -1.23,\n",
    "    'I': -4.81, 'K': 3.88, 'L': -4.68, 'M': -3.66, 'N': 0.96, 'P': 0.75, 'Q': 1.52,\n",
    "    'R': 2.11, 'S': 1.74, 'T': 0.78, 'V': -3.5, 'W': -3.32, 'Y': -1.01,\n",
    "    'X': 0, 'U': 0, 'B': 0, 'J': 0, 'O': 0, 'Z': 0\n",
    "}\n",
    "\n",
    "# Convert all sequences to numerical arrays using the custom mapping\n",
    "# Assuming you have already loaded the merged_sequences list and defined the custom mapping\n",
    "\n",
    "# Convert all sequences to numerical arrays using the custom mapping\n",
    "mapped_sequences = []\n",
    "for sequence in merged_sequences:\n",
    "    mapped_sequence = [mapping[aa] for aa in sequence]\n",
    "    mapped_sequences.append(mapped_sequence)\n",
    "\n",
    "print(\"Total number of sequences:\" ,len(mapped_sequences))\n",
    "print(\"First mapped sequence:\",mapped_sequences[0])\n",
    "print(\"Length of each sequence:\", len(mapped_sequences[0]))\n",
    "print(len(merged_labels))\n",
    "\n",
    "\n",
    "# Save the numerical sequences and labels to a new fasta-like file with numerical representations\n",
    "output_file = \"Mapped.fasta\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    for i, (mapped_sequence, label) in enumerate(zip(mapped_sequences, merged_labels)):\n",
    "        file.write(f\"Label: {label}\\n\")\n",
    "        file.write(\"Sequence: \")\n",
    "        file.write(\" \".join(map(str, mapped_sequence)))  \n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Numeric sequences and labels saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a3fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 5259\n",
      "Validation dataset size: 657\n",
      "Test dataset size: 658\n",
      "Sample Sequence Tensor:\n",
      "tensor([-6.0400,  1.5200, -3.5000, -3.5000,  2.3000,  2.1100,  0.0000, -4.6800,\n",
      "         1.3600, -0.2100,  0.0000, -0.2100,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n",
      "Tensor shape: torch.Size([750])\n",
      "First label tensor: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "with open(\"Mapped.fasta\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "dataset = ProteinDataset(mapped_sequences, merged_labels)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print('Train dataset size:', len(train_dataset)) \n",
    "print('Validation dataset size:', len(val_dataset)) \n",
    "print('Test dataset size:', len(test_dataset))\n",
    "\n",
    "for sequence_tensor, label_tensor in train_loader:\n",
    "    print(\"Sample Sequence Tensor:\")\n",
    "    print(sequence_tensor[0])  # Printing the first tensor in the batch\n",
    "    print(\"Tensor shape:\",sequence_tensor[0].shape)\n",
    "    print(\"First label tensor:\",label_tensor[0])\n",
    "\n",
    "    break  # Stop after printing one batch to avoid excessive output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573dc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use padding as the dimensions in each conv layer reduce by 4 every time such that:\n",
    "# 128*496\n",
    "# 128*492\n",
    "# 128*488\n",
    "\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_length=750,classes=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #First layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5,stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        #Second layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5,padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        #Third layer\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5,padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear((128//2)*(input_length-4), 256) # //2 from stride in first convolutional layer\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, classes)\n",
    "        \n",
    "     \n",
    "\n",
    "    def forward(self, input):\n",
    "        # Apply the convolutional layers\n",
    "        input = input.view(input.size(0), 1, -1)\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    " \n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.dropout2(output)\n",
    "\n",
    " \n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.dropout3(output)\n",
    "        \n",
    "        output = output.view(output.size(0), -1)\n",
    "        \n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.dropout4(output)\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        # Apply sigmoid activation to the output of the last fully connected layer\n",
    "        output = torch.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a55d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 35.801365658922016\n",
      "Epoch 2/100, Loss: 1.810501588965362\n",
      "Epoch 3/100, Loss: 1.5492285152651228\n",
      "Epoch 4/100, Loss: 1.4568791580650042\n",
      "Epoch 5/100, Loss: 1.4019871185410697\n",
      "Epoch 6/100, Loss: 1.3141268086883258\n",
      "Epoch 7/100, Loss: 1.192051281344216\n",
      "Epoch 8/100, Loss: 1.1158672931059352\n",
      "Epoch 9/100, Loss: 1.0641243536517304\n",
      "Epoch 10/100, Loss: 1.0184454850430757\n",
      "Epoch 11/100, Loss: 0.8549816124844101\n",
      "Epoch 12/100, Loss: 0.8020308591284842\n",
      "Epoch 13/100, Loss: 0.71794286482739\n",
      "Epoch 14/100, Loss: 0.6359044442761619\n",
      "Epoch 15/100, Loss: 0.5485228221371489\n",
      "Epoch 16/100, Loss: 0.4638634093527524\n",
      "Epoch 17/100, Loss: 0.48626745871777804\n",
      "Epoch 18/100, Loss: 0.41477167072161186\n",
      "Epoch 19/100, Loss: 0.3590529552608166\n",
      "Epoch 20/100, Loss: 0.3326034056690504\n",
      "Epoch 21/100, Loss: 0.2914390876045767\n",
      "Epoch 22/100, Loss: 0.24130406610245975\n",
      "Epoch 23/100, Loss: 0.26521472492307985\n",
      "Epoch 24/100, Loss: 0.24219680479112662\n",
      "Epoch 25/100, Loss: 0.20468068207209966\n",
      "Epoch 26/100, Loss: 0.20609264370967756\n",
      "Epoch 27/100, Loss: 0.17283826244046102\n",
      "Epoch 28/100, Loss: 0.17909785609622048\n",
      "Epoch 29/100, Loss: 0.15281053056132118\n",
      "Epoch 30/100, Loss: 0.1511973780140562\n",
      "Epoch 31/100, Loss: 0.15014194827175364\n",
      "Epoch 32/100, Loss: 0.14190565843908293\n",
      "Epoch 33/100, Loss: 0.14303385093808174\n",
      "Epoch 34/100, Loss: 0.12897738445339338\n",
      "Epoch 35/100, Loss: 0.13981300831403373\n",
      "Epoch 36/100, Loss: 0.17739275203279728\n",
      "Epoch 37/100, Loss: 0.14965373312808433\n",
      "Epoch 38/100, Loss: 0.10875941928968115\n",
      "Epoch 39/100, Loss: 0.11554879132869109\n",
      "Epoch 40/100, Loss: 0.14161430197363756\n",
      "Epoch 41/100, Loss: 0.08481044722896702\n",
      "Epoch 42/100, Loss: 0.11349992366191351\n",
      "Epoch 43/100, Loss: 0.17219631264935126\n",
      "Epoch 44/100, Loss: 0.09583556891049978\n",
      "Epoch 45/100, Loss: 0.08476541177281793\n",
      "Epoch 46/100, Loss: 0.0730165811782738\n",
      "Epoch 47/100, Loss: 0.06417332461349806\n",
      "Epoch 48/100, Loss: 0.07620042001933984\n",
      "Epoch 49/100, Loss: 0.08300424432206266\n",
      "Epoch 50/100, Loss: 0.08186683554272607\n",
      "Epoch 51/100, Loss: 0.08068628881548373\n",
      "Epoch 52/100, Loss: 0.09227416102053984\n",
      "Epoch 53/100, Loss: 0.061802347480618164\n",
      "Epoch 54/100, Loss: 0.09778415662113507\n",
      "Epoch 55/100, Loss: 0.09329521149959204\n",
      "Epoch 56/100, Loss: 0.05104239757682355\n",
      "Epoch 57/100, Loss: 0.060842589932089707\n",
      "Epoch 58/100, Loss: 0.061071737089528226\n",
      "Epoch 59/100, Loss: 0.06065852189752853\n",
      "Epoch 60/100, Loss: 0.056983953130976206\n",
      "Epoch 61/100, Loss: 0.07510774412174832\n",
      "Epoch 62/100, Loss: 0.11505178288328198\n",
      "Epoch 63/100, Loss: 0.09694843676011518\n",
      "Epoch 64/100, Loss: 0.08815578127033868\n",
      "Epoch 65/100, Loss: 0.07499822949604043\n",
      "Epoch 66/100, Loss: 0.06336698002832115\n",
      "Epoch 67/100, Loss: 0.08975575309035913\n",
      "Epoch 68/100, Loss: 0.06432099889893576\n",
      "Epoch 69/100, Loss: 0.06567724988321369\n",
      "Epoch 70/100, Loss: 0.05721403480032986\n",
      "Epoch 71/100, Loss: 0.04946504942721353\n",
      "Epoch 72/100, Loss: 0.08669274304729868\n",
      "Epoch 73/100, Loss: 0.10253971368777302\n",
      "Epoch 74/100, Loss: 0.06315712993613111\n",
      "Epoch 75/100, Loss: 0.07910657668802536\n",
      "Epoch 76/100, Loss: 0.06937370345629049\n",
      "Epoch 77/100, Loss: 0.05712062803114641\n",
      "Epoch 78/100, Loss: 0.045456919590678976\n",
      "Epoch 79/100, Loss: 0.0548258237980025\n",
      "Epoch 80/100, Loss: 0.052283990311102486\n",
      "Epoch 81/100, Loss: 0.051222335670213656\n",
      "Epoch 82/100, Loss: 0.06311419667711235\n",
      "Epoch 83/100, Loss: 0.0819155333370392\n",
      "Epoch 84/100, Loss: 0.08612276170894785\n",
      "Epoch 85/100, Loss: 0.057439661514506024\n",
      "Epoch 86/100, Loss: 0.038859351318470146\n",
      "Epoch 87/100, Loss: 0.03547499031004197\n",
      "Epoch 88/100, Loss: 0.04602905181412286\n",
      "Epoch 89/100, Loss: 0.04428053149988629\n",
      "Epoch 90/100, Loss: 0.04322378808316195\n",
      "Epoch 91/100, Loss: 0.05239903687648827\n",
      "Epoch 92/100, Loss: 0.057795655693999436\n",
      "Epoch 93/100, Loss: 0.04089225152560141\n",
      "Epoch 94/100, Loss: 0.02820338756021745\n",
      "Epoch 95/100, Loss: 0.0393999313303041\n",
      "Epoch 96/100, Loss: 0.039917881582986633\n",
      "Epoch 97/100, Loss: 0.04496991757813068\n",
      "Epoch 98/100, Loss: 0.03835106499159252\n",
      "Epoch 99/100, Loss: 0.04726541998251429\n",
      "Epoch 100/100, Loss: 0.06338728706457845\n",
      "Validation Accuracy: 0.8356164383561644, Validation MCC: 0.6019411209512012\n",
      "Test Accuracy: 0.7917933130699089, Test MCC: 0.483632734830997\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "# Loss and optimizer functions\n",
    "weights = [3.0]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "criterion = nn.BCELoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "# 4. Train the Model\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sequence_tensor, label_tensor in train_loader:\n",
    "        sequence_tensor, label_tensor = sequence_tensor.to(device), label_tensor.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(sequence_tensor.unsqueeze(1))  # Add a channel dimension for Conv1d\n",
    "        loss = criterion(outputs, label_tensor.unsqueeze(1))  # Add a channel dimension for BCELoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "    \n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    true_positives, true_negatives, false_positives, false_negatives = 0, 0, 0, 0\n",
    "    all_predicted_labels = []  # Initialize the list\n",
    "    all_true_labels = []  # Initialize the list\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequence_tensor, label_tensor in data_loader:\n",
    "            sequence_tensor, label_tensor = sequence_tensor.to(device), label_tensor.to(device)\n",
    "\n",
    "            outputs = model(sequence_tensor.unsqueeze(1))\n",
    "            predicted_labels = outputs >= 0.5\n",
    "\n",
    "            correct_predictions += (predicted_labels == label_tensor.unsqueeze(1)).sum().item()\n",
    "            total_samples += label_tensor.size(0)\n",
    "\n",
    "            # Calculate true positives, true negatives, false positives, and false negatives for this batch\n",
    "            true_positives += ((predicted_labels == 1) & (label_tensor.unsqueeze(1) == 1)).sum().item()\n",
    "            true_negatives += ((predicted_labels == 0) & (label_tensor.unsqueeze(1) == 0)).sum().item()\n",
    "            false_positives += ((predicted_labels == 1) & (label_tensor.unsqueeze(1) == 0)).sum().item()\n",
    "            false_negatives += ((predicted_labels == 0) & (label_tensor.unsqueeze(1) == 1)).sum().item()\n",
    "\n",
    "            # Calculate MCC for this batch and store the predicted and true labels\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "            all_true_labels.extend(label_tensor.unsqueeze(1).cpu().numpy())\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient (MCC) using scikit-learn\n",
    "    mcc = matthews_corrcoef(all_true_labels, all_predicted_labels)\n",
    "\n",
    "    return accuracy, mcc\n",
    "\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_accuracy, val_mcc = evaluate_model(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy}, Validation MCC: {val_mcc}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy, test_mcc = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy}, Test MCC: {test_mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d744b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
